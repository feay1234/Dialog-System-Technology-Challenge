{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss, BCELoss\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, BertModel, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel\n",
    "class DST_SPAN(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.clf = nn.Linear(config.hidden_size, 3)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        slot_label=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        rCLS = sequence_output[:, 0, :]\n",
    "        slot_logits = self.clf(rCLS)\n",
    "\n",
    "        outputs = (start_logits, end_logits,) + outputs[2:]\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            \n",
    "            loss_bce = BCELoss()\n",
    "            slot_loss = loss_fct(slot_logits, slot_label)\n",
    "            \n",
    "            total_loss = (start_loss + end_loss + slot_loss) / 3\n",
    "            outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing DST_SPAN: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DST_SPAN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DST_SPAN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DST_SPAN were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias', 'clf.weight', 'clf.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DST_SPAN.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(context, question, value):\n",
    "    max_len = 512\n",
    "    tokenized_context = tokenizer(context)\n",
    "    tokenized_question = tokenizer(question)\n",
    "    tokenized_value = tokenizer(value)['input_ids'][1:-1]\n",
    "\n",
    "    # Create inputs\n",
    "    input_ids = tokenized_context['input_ids'] + tokenized_question['input_ids'][1:]\n",
    "    token_type_ids = [0] * len(tokenized_context['input_ids']) + [1] * len(\n",
    "        tokenized_question['input_ids'][1:]\n",
    "    )\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Pad and create attention masks.\n",
    "    # Skip if truncation is needed\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:  # pad\n",
    "        input_ids = input_ids + ([0] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "\n",
    "    # Find start and stop position of span\n",
    "    l = len(tokenized_value)\n",
    "    start, end = -1, -1\n",
    "    for i in range(len(input_ids), l, -1):\n",
    "        if input_ids[i-l: i] == [0] * l:\n",
    "            continue\n",
    "        elif input_ids[i-l: i] == tokenized_value:\n",
    "            start = i-l\n",
    "            end = i-1\n",
    "            break\n",
    "    return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask), start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = preprocess(\"book a hotel on monday\", \"hotel-day\", \"monday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.639381408691406\n",
      "4.377444744110107\n",
      "4.268583297729492\n",
      "4.237524032592773\n",
      "4.354714870452881\n",
      "4.079228401184082\n",
      "4.131597995758057\n",
      "4.092955589294434\n",
      "4.08426570892334\n",
      "4.602624416351318\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for i in range(10):\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor([batch[0]]+[batch[0]]),\n",
    "        \"attention_mask\": torch.tensor([batch[1]]+[batch[1]]),\n",
    "        \"token_type_ids\": torch.tensor([batch[2]]+[batch[2]]),\n",
    "        \"start_positions\": torch.tensor([batch[3], batch[3]]),\n",
    "        \"end_positions\": torch.tensor([batch[4], batch[4]]),\n",
    "        \"slot_label\": torch.tensor([2,2]),\n",
    "    }\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs[0]\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['end_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.1427e-02,  7.1057e-02, -2.6099e-01, -6.7041e-02,  6.6192e-02,\n",
       "           3.0436e-01, -1.3487e-01, -1.4287e-01,  5.1434e-01,  1.0080e-01,\n",
       "          -1.3572e-01,  4.6623e-02,  1.8986e-02,  1.6334e-02,  4.4456e-03,\n",
       "           9.9585e-03,  4.9683e-02,  8.7223e-02, -1.3443e-02, -5.6302e-03,\n",
       "           4.2708e-03,  4.0149e-02,  1.5471e-01,  6.4796e-03,  2.0075e-02,\n",
       "           3.4052e-02,  3.6993e-02,  8.3071e-02,  1.7934e-02,  1.1810e-02,\n",
       "           3.1950e-02,  1.7915e-01,  5.9460e-02,  1.1314e-01,  8.0607e-03,\n",
       "           3.0791e-02,  3.8122e-02,  1.7604e-02,  3.7104e-02,  9.3630e-03,\n",
       "           2.0538e-02,  2.6220e-03, -5.2061e-03,  6.9189e-03, -1.2484e-02,\n",
       "          -1.1577e-03,  1.6699e-01,  1.7600e-01,  3.5491e-02, -2.1768e-03,\n",
       "           2.9017e-03,  3.8176e-02,  4.7572e-02,  3.5803e-02,  3.1142e-02,\n",
       "           2.5433e-02,  3.0105e-02,  1.2157e-01,  1.1454e-02,  2.7681e-03,\n",
       "           7.3168e-02,  1.8511e-01,  3.5458e-02,  6.2214e-02, -1.3537e-02,\n",
       "           3.8877e-03,  2.7923e-02,  2.9575e-02,  1.7109e-02,  7.2488e-03,\n",
       "           1.4272e-02,  3.5307e-02,  3.4296e-03, -1.4394e-03, -1.3041e-02,\n",
       "           2.1418e-02,  2.1145e-01,  1.3633e-02,  4.7579e-03,  4.7406e-03,\n",
       "           3.3832e-02,  5.5210e-02,  2.1963e-02,  2.3262e-02,  2.7476e-02,\n",
       "           3.0690e-02,  6.0037e-02,  1.6299e-01, -1.0168e-02,  1.0274e-02,\n",
       "           1.8841e-01,  5.1331e-02,  1.4346e-01, -9.8567e-03,  4.2381e-03,\n",
       "           2.9332e-02,  6.6379e-04,  1.0426e-02,  8.2068e-03, -1.2152e-02,\n",
       "          -5.1015e-03,  1.1799e-01, -1.4897e-03, -1.6717e-02,  3.5415e-03,\n",
       "           1.4164e-01,  4.6810e-02,  1.2683e-01,  2.6609e-03,  3.4516e-02,\n",
       "           2.2907e-02,  2.0482e-02,  4.4368e-02,  1.5933e-02,  2.4683e-02,\n",
       "           1.0226e-01,  1.7889e-01,  2.8103e-02,  1.0955e-01, -8.9467e-03,\n",
       "           1.4065e-02,  1.0066e-01,  7.1627e-04,  8.3628e-03,  1.8648e-02,\n",
       "           1.6511e-02,  3.1838e-02, -2.3200e-03, -2.8879e-03,  2.3117e-03,\n",
       "           7.5685e-02,  5.8201e-02,  1.5465e-01, -1.0649e-02,  1.6135e-02,\n",
       "           2.1732e-01,  3.6852e-02,  1.2869e-01,  6.3507e-03,  2.5619e-02,\n",
       "           6.3703e-02,  7.8860e-02,  1.8780e-02,  2.0572e-02,  1.8082e-02,\n",
       "           1.7999e-02,  9.5658e-03,  1.6715e-01,  1.6230e-01,  3.9067e-02,\n",
       "           1.7605e-03,  1.6951e-01,  5.4441e-02,  1.6018e-01,  1.4733e-02,\n",
       "          -1.0895e-02,  1.2523e-02,  9.5511e-03,  7.6526e-03,  5.9528e-03,\n",
       "           1.1298e-03, -1.0074e-03,  5.9817e-02, -1.3279e-02, -1.5173e-02,\n",
       "          -1.1551e-02, -1.5115e-02,  1.3175e-02,  5.8660e-02,  8.6580e-03,\n",
       "           2.4022e-02,  2.6500e-02,  1.2807e-02,  2.3131e-02,  1.5113e-02,\n",
       "           3.2497e-02,  1.4979e-01,  1.8061e-01,  2.5129e-02,  1.9494e-02,\n",
       "           1.9532e-01,  2.0782e-01,  1.9988e-01,  1.0115e-03, -2.6836e-03,\n",
       "           9.2587e-02,  2.9994e-02,  1.8666e-01,  3.9861e-07,  1.0315e-02,\n",
       "           3.0492e-02,  1.7242e-02,  7.2873e-04,  1.5045e-02,  1.3889e-02,\n",
       "          -8.7738e-03,  2.8415e-03,  1.6327e-02,  1.7638e-01,  1.4454e-01,\n",
       "           3.3975e-02,  7.7754e-02,  1.6596e-01,  2.7119e-03,  2.3314e-03,\n",
       "           3.2928e-02,  9.3171e-02,  7.3553e-02,  5.0046e-02,  1.9865e-02,\n",
       "           3.6915e-02,  6.4092e-02,  8.5530e-02,  1.8882e-01,  8.3347e-03,\n",
       "           2.6534e-03,  1.7082e-01,  3.5798e-02,  1.5808e-01,  5.9596e-03,\n",
       "           2.5999e-02,  2.3348e-02,  1.4474e-02,  1.0092e-02,  1.3746e-02,\n",
       "           4.8287e-02,  1.0654e-02, -1.2782e-02, -5.8498e-03,  1.3171e-01,\n",
       "           1.5067e-01,  2.6370e-02,  1.1445e-01, -2.5698e-02, -8.5070e-03,\n",
       "           1.5059e-02,  2.1938e-02,  6.0843e-02,  7.1597e-03,  1.8444e-02,\n",
       "           1.1743e-02,  4.0422e-03,  7.1942e-03,  3.0714e-02, -1.0207e-02,\n",
       "           5.8945e-03,  4.9752e-02,  1.7909e-01,  5.0088e-02,  2.0430e-01,\n",
       "           7.9154e-03,  2.9134e-02,  4.4338e-02,  1.0394e-01,  5.4899e-02,\n",
       "           1.5282e-02,  4.3958e-03,  4.3746e-02,  1.7298e-01,  1.9752e-01,\n",
       "           1.1147e-01,  3.5945e-03,  5.0338e-02,  1.2119e-02,  9.1007e-03,\n",
       "           3.2821e-02,  9.0872e-03,  1.8610e-02,  8.8425e-03, -7.5358e-04,\n",
       "          -1.2067e-02, -1.2211e-02, -7.3733e-03, -3.7004e-03,  4.8175e-03,\n",
       "           1.8543e-01,  1.7851e-01,  1.2183e-02, -1.3697e-02, -6.7574e-03,\n",
       "           4.6761e-02,  4.3273e-02,  1.3199e-02, -1.1127e-03,  2.5193e-02,\n",
       "           3.4760e-02,  9.2761e-03,  2.3064e-02,  2.6127e-02,  1.0723e-02,\n",
       "           2.4830e-02,  4.7145e-02,  1.7128e-01, -9.0252e-03,  1.0942e-02,\n",
       "          -1.6705e-02,  6.6268e-02,  2.0400e-01,  2.1843e-02,  1.9869e-01,\n",
       "          -2.7990e-02, -1.8984e-02,  3.7470e-03,  3.1023e-02,  2.8794e-02,\n",
       "           1.0620e-02,  1.7388e-02,  1.0170e-03,  8.9325e-03,  8.9580e-03,\n",
       "          -1.6644e-03, -9.0026e-03,  4.2993e-02,  1.0310e-02, -5.1502e-03,\n",
       "           1.9847e-02,  4.8644e-02,  2.9657e-02,  1.0502e-01,  8.4750e-02,\n",
       "          -1.0702e-02, -7.5613e-03,  8.4045e-03,  2.3093e-02,  2.2533e-02,\n",
       "           7.5137e-03,  9.6557e-03,  1.4480e-02,  2.0104e-04,  1.2756e-02,\n",
       "           1.4548e-01, -2.5071e-02, -9.8890e-03,  1.9259e-01,  8.4491e-02,\n",
       "           6.0766e-02, -1.0101e-02, -5.0171e-03,  1.7744e-02,  1.1910e-01,\n",
       "           2.9626e-02,  2.2898e-02,  2.9471e-02,  5.0241e-02,  1.8038e-01,\n",
       "           1.4233e-01,  4.1552e-03, -3.4201e-03,  2.8292e-02,  1.9129e-01,\n",
       "           1.2950e-01,  1.5050e-02,  1.0007e-01, -1.3071e-02,  7.7845e-03,\n",
       "           8.7286e-02, -6.3041e-04,  2.9118e-03, -1.1112e-02,  1.4794e-02,\n",
       "           1.3694e-01, -1.5052e-02, -8.0216e-03,  1.5445e-02,  1.1724e-02,\n",
       "           3.2000e-02,  3.9750e-02,  1.5423e-02,  2.7769e-02,  2.2652e-02,\n",
       "           7.2788e-02,  6.3591e-02,  2.4146e-02,  4.8853e-03,  3.0530e-02,\n",
       "           2.0036e-01,  3.9504e-02,  1.7844e-01, -1.3628e-02,  1.3792e-02,\n",
       "           5.0059e-02,  6.0152e-03,  2.1256e-02,  2.1283e-02,  6.9876e-03,\n",
       "           3.9220e-03,  8.2440e-03, -1.5224e-03,  3.7729e-03,  8.3758e-02,\n",
       "           1.7764e-01,  1.5167e-02,  5.6422e-02,  1.5753e-01,  5.9382e-02,\n",
       "           2.0898e-01,  7.8570e-02, -1.2612e-03,  1.6734e-01,  8.9635e-02,\n",
       "           2.0343e-01, -1.3011e-02, -1.1616e-02,  1.0467e-02,  2.1238e-03,\n",
       "           9.1757e-03, -7.3493e-03, -1.8614e-02, -4.0157e-03, -4.1534e-04,\n",
       "           2.4309e-02,  4.1543e-02, -1.1365e-02,  9.4224e-03,  7.3993e-02,\n",
       "           5.3622e-02,  1.5200e-01,  4.0697e-03,  3.7194e-02,  1.7242e-01,\n",
       "           1.9214e-02,  1.8188e-02,  1.1758e-01,  1.5237e-01,  1.5866e-01,\n",
       "           3.2695e-03,  1.0210e-02,  1.5659e-01,  1.2384e-01, -6.9722e-04,\n",
       "          -3.0658e-03,  7.5860e-03,  2.1269e-02, -4.0571e-03,  6.9251e-04,\n",
       "          -3.6474e-03, -1.0946e-02,  3.6833e-04,  2.4611e-02,  1.8960e-02,\n",
       "           4.4808e-03, -7.4051e-03,  1.8624e-03, -2.0077e-03,  9.4605e-03,\n",
       "          -2.4779e-02, -1.3807e-02,  1.8976e-02,  4.1121e-02, -1.0705e-02,\n",
       "           3.5035e-03,  1.7099e-02,  3.4272e-02,  5.1049e-02,  2.3931e-02,\n",
       "           3.1082e-02,  2.0548e-02,  7.4577e-03,  1.5878e-02,  2.2351e-02,\n",
       "           5.2159e-04,  5.1133e-02,  1.9293e-01,  1.1619e-02,  1.7378e-01,\n",
       "           1.6349e-01,  2.3635e-02,  1.8340e-01, -1.2759e-02,  1.4427e-02,\n",
       "           4.8601e-02,  2.0374e-02,  5.6461e-03,  8.8098e-03,  1.4295e-02,\n",
       "           2.3708e-02,  3.9561e-02, -1.3785e-02, -8.9256e-03,  4.4037e-03,\n",
       "          -3.9425e-03,  1.3058e-01,  2.7285e-02,  1.4252e-02,  6.4058e-03,\n",
       "           1.5663e-02,  3.8385e-02,  2.7513e-02,  2.9520e-02,  2.3684e-02,\n",
       "           1.9047e-02,  3.0539e-02,  1.6857e-01,  8.7604e-02,  1.1211e-01,\n",
       "           1.7810e-02, -1.4583e-02,  1.4929e-02,  1.6228e-01,  1.4559e-02,\n",
       "           3.4141e-02,  1.3044e-01,  4.7652e-03,  2.1170e-02,  1.8729e-02,\n",
       "           2.5704e-02,  1.3042e-02, -7.5087e-03,  2.2630e-02,  1.7792e-01,\n",
       "           2.6275e-02,  8.4006e-03]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-6.8197e-02, -6.7729e-02, -1.6321e-01,  2.2652e-01,  2.5252e-02,\n",
       "          -2.0243e-02,  4.6355e-02,  9.4317e-02, -1.6353e-01, -2.2116e-01,\n",
       "           4.9074e-02, -1.6985e-02, -2.3960e-02, -4.9989e-02, -2.9360e-02,\n",
       "          -4.5261e-02,  4.9848e-02,  9.0045e-02, -5.3409e-02, -5.5378e-02,\n",
       "          -4.4422e-02,  5.3073e-02, -2.8918e-02, -3.9252e-02, -4.9670e-02,\n",
       "          -4.1352e-02, -5.2255e-02,  4.1879e-02, -4.4444e-02, -6.8711e-02,\n",
       "          -2.1627e-02,  1.2846e-01,  7.7323e-02, -2.3092e-02, -5.6889e-02,\n",
       "          -6.3767e-02, -4.0378e-02,  3.1844e-02, -3.7843e-02, -2.3877e-02,\n",
       "          -4.5673e-02, -1.2890e-02,  3.4907e-03, -4.9075e-02, -2.1565e-02,\n",
       "          -5.6492e-02,  1.2003e-01,  1.2734e-01,  3.8250e-02, -3.9147e-02,\n",
       "          -3.4551e-02, -5.6534e-02, -9.2984e-03, -4.8250e-02, -5.4642e-02,\n",
       "          -2.9760e-02, -4.8921e-02,  7.9904e-02, -4.9627e-02, -7.6701e-02,\n",
       "          -2.7899e-02,  9.0634e-02,  4.4769e-02, -3.5820e-02, -3.6425e-02,\n",
       "          -4.3239e-02, -2.6953e-02, -2.2375e-02, -3.3815e-02, -2.7381e-02,\n",
       "          -4.9982e-02,  3.8625e-02,  8.0372e-02, -5.8284e-02, -2.4113e-02,\n",
       "          -5.6340e-02,  9.8384e-02, -1.7373e-02, -5.2529e-02, -2.9737e-02,\n",
       "          -4.2999e-02, -2.3672e-04, -2.8606e-02, -4.8492e-02, -6.1241e-02,\n",
       "           4.8913e-03,  9.6637e-02,  1.8396e-02, -4.5837e-02, -7.1915e-02,\n",
       "           7.2750e-02,  6.3379e-02, -5.5463e-03, -2.8814e-02, -3.3412e-02,\n",
       "          -4.1500e-02, -1.7557e-02, -3.8434e-02, -4.7804e-02, -2.2998e-02,\n",
       "          -1.7972e-02,  1.1862e-01, -2.9925e-02, -5.0815e-02, -5.6671e-02,\n",
       "           8.6035e-02,  6.8445e-02, -4.1054e-02, -2.2233e-02, -5.0688e-02,\n",
       "          -2.3161e-02,  5.2130e-04, -5.0311e-02, -3.9261e-02, -5.7419e-02,\n",
       "           4.6670e-02,  1.2456e-01,  6.5201e-02, -2.3961e-02, -3.7144e-02,\n",
       "          -2.5200e-02, -1.8950e-02, -2.6214e-02, -3.5978e-02, -2.5564e-02,\n",
       "          -3.7168e-02,  4.1120e-02, -1.8043e-02, -3.9259e-02, -4.3173e-02,\n",
       "           3.6314e-02,  1.1236e-01, -1.8086e-02, -4.1293e-02, -8.0552e-02,\n",
       "           4.9818e-02,  2.3546e-02, -5.4306e-02, -4.2450e-02, -4.4967e-02,\n",
       "          -5.0511e-02, -2.1757e-02, -4.2735e-02, -5.3305e-02, -4.0405e-02,\n",
       "          -3.0701e-02, -7.2715e-02,  7.7419e-02,  8.7597e-02, -2.6701e-02,\n",
       "          -6.1243e-02,  5.0861e-02,  8.7999e-02, -1.0803e-02, -3.2978e-02,\n",
       "          -2.8652e-02, -3.5870e-02, -2.4150e-02, -3.0432e-02, -3.4163e-02,\n",
       "          -3.3415e-02, -3.7390e-02,  6.0832e-02, -2.7941e-02, -6.1234e-02,\n",
       "          -4.0780e-02, -3.6219e-02, -5.3380e-02, -3.5510e-02, -2.6132e-02,\n",
       "          -3.6945e-02, -4.2006e-02, -2.8336e-02, -5.1355e-02, -4.3560e-02,\n",
       "          -5.6240e-02,  1.0429e-01,  3.1641e-02, -5.5284e-02, -6.7090e-02,\n",
       "           6.9149e-02,  7.8148e-02, -2.2987e-02, -3.5362e-02, -6.1827e-02,\n",
       "          -4.1386e-02,  3.5181e-02, -7.2493e-03, -2.8186e-02, -3.1129e-02,\n",
       "          -3.6102e-02, -1.9461e-02, -2.9738e-02, -3.7395e-02, -5.2029e-02,\n",
       "          -2.4184e-02, -5.2606e-02, -5.3369e-02,  1.2890e-01,  1.4079e-01,\n",
       "          -4.6730e-02, -1.0901e-02, -3.2171e-02, -4.0440e-02, -5.6166e-02,\n",
       "          -5.3955e-02, -1.1950e-02,  1.3938e-02, -5.5565e-02, -2.8142e-02,\n",
       "          -6.6816e-02,  1.8581e-02,  6.7772e-02,  2.3842e-02, -5.6454e-02,\n",
       "          -6.8388e-02,  2.1955e-02,  4.7774e-02, -2.2118e-02, -2.4250e-02,\n",
       "          -4.0812e-02, -3.0526e-02, -3.9858e-02, -3.7184e-02, -4.5469e-02,\n",
       "           6.2735e-02, -2.1424e-02, -3.4660e-02, -5.9630e-02,  9.9726e-02,\n",
       "           1.4168e-01,  6.0777e-02, -3.0735e-02, -3.0934e-02, -5.5592e-02,\n",
       "          -3.4829e-02, -3.6377e-02, -5.5541e-02, -2.1261e-02, -2.8766e-02,\n",
       "          -4.2592e-02, -1.1887e-02,  5.4058e-02, -3.4166e-02, -2.3119e-02,\n",
       "          -6.4213e-02, -4.5636e-02,  8.6522e-02,  5.9350e-02, -2.4472e-02,\n",
       "          -2.5922e-02, -4.4655e-02, -5.1080e-02,  5.9473e-02,  8.0826e-02,\n",
       "          -7.4318e-02, -4.8997e-02, -3.0367e-02,  1.2508e-01,  2.6060e-02,\n",
       "          -2.8715e-02, -5.3514e-02, -7.0719e-02, -3.8206e-02, -4.0429e-02,\n",
       "          -5.3577e-02, -2.7472e-02, -3.6932e-02, -4.3701e-02, -3.7888e-02,\n",
       "          -3.5155e-02, -3.2048e-02, -4.9976e-02, -4.1105e-02, -4.2589e-02,\n",
       "           1.2656e-01,  8.6082e-02, -1.0150e-02, -5.8649e-02, -4.5062e-02,\n",
       "           6.8326e-02,  5.2041e-02, -4.4321e-02, -2.2983e-02, -3.8436e-02,\n",
       "          -5.0272e-02, -2.8533e-02, -4.7354e-02, -6.3353e-02, -3.4444e-02,\n",
       "          -6.5953e-02,  2.6275e-02,  1.3982e-01,  4.0410e-02, -7.1897e-02,\n",
       "          -4.2944e-02, -7.0248e-03,  6.9748e-02,  5.7680e-02, -3.4748e-03,\n",
       "          -2.4810e-02, -3.8988e-02, -4.1976e-02, -4.6776e-02, -3.2518e-02,\n",
       "          -3.0880e-02, -4.4019e-02, -3.9419e-02, -4.5011e-02, -5.5156e-02,\n",
       "          -3.0422e-02, -2.4767e-02,  4.7918e-03, -4.5426e-02, -8.1043e-02,\n",
       "          -8.2705e-02, -5.2208e-02,  3.8546e-02,  1.6610e-02, -5.7574e-02,\n",
       "          -4.1327e-02, -5.2638e-02, -4.3243e-02, -4.7641e-02, -5.1585e-02,\n",
       "          -3.8456e-02, -4.2779e-02, -5.8087e-02, -8.2195e-03,  8.6574e-02,\n",
       "           1.8413e-02, -4.0664e-02, -5.2422e-02,  1.0558e-01,  1.0571e-01,\n",
       "          -2.2508e-02, -5.5630e-02, -3.7381e-02, -4.3738e-02,  7.0883e-03,\n",
       "          -3.5838e-02, -4.1528e-02, -5.0231e-02, -3.1068e-02,  1.4134e-01,\n",
       "           3.4382e-02, -5.1719e-02, -4.5936e-02, -7.8324e-02,  9.9799e-02,\n",
       "           6.5909e-02,  5.2182e-02, -4.3002e-02, -3.5526e-02, -4.7112e-02,\n",
       "           5.6203e-02,  1.7988e-03, -5.4576e-02, -2.9910e-02, -5.1283e-02,\n",
       "           1.6354e-02, -3.3296e-02, -3.1763e-02, -4.0195e-02, -3.1553e-02,\n",
       "          -4.1578e-02, -4.3317e-02, -2.5460e-02, -4.6398e-02, -4.6571e-02,\n",
       "           3.7770e-02,  9.0092e-02, -6.4080e-02, -4.7108e-02, -6.6287e-02,\n",
       "           1.0698e-01,  4.7466e-02, -2.1908e-02, -3.4895e-02, -5.4022e-02,\n",
       "          -5.2810e-02, -3.0914e-02, -4.6419e-02, -3.9074e-02, -3.6403e-02,\n",
       "          -3.9720e-03, -4.7895e-02, -4.7983e-02, -3.3841e-02,  7.1922e-02,\n",
       "           1.2414e-01, -1.2748e-03, -5.9147e-02,  6.5077e-02,  8.4125e-02,\n",
       "          -9.9264e-03, -4.2532e-02, -7.6118e-02, -5.8086e-03,  5.3717e-02,\n",
       "          -2.6235e-02, -3.0955e-02, -4.7155e-02, -4.3671e-02, -3.5018e-02,\n",
       "          -4.8402e-02, -3.8021e-02, -2.7569e-02, -3.5734e-02, -5.0561e-02,\n",
       "           3.2330e-03,  4.6987e-02, -8.7112e-02, -6.8862e-02, -3.1273e-02,\n",
       "           2.9031e-02, -5.1663e-02, -4.0418e-02, -6.3831e-02, -1.8320e-02,\n",
       "          -4.5305e-02, -5.3555e-02,  2.6669e-02,  8.9340e-02,  1.4818e-04,\n",
       "          -5.5445e-02, -5.9672e-02, -8.5902e-03, -1.4091e-02, -3.9802e-02,\n",
       "          -3.6902e-02, -2.4864e-02, -4.4669e-02, -2.6358e-02, -3.7720e-02,\n",
       "          -5.1772e-02, -2.5390e-02, -4.6585e-02,  1.1769e-02,  2.5576e-02,\n",
       "          -6.1066e-02, -3.0869e-02, -3.8156e-02, -2.8321e-02, -2.1071e-03,\n",
       "          -5.2205e-02, -5.0991e-02, -5.9091e-02,  6.6003e-02, -3.8527e-02,\n",
       "          -4.6965e-02, -3.8740e-02, -4.8217e-02, -5.5268e-02, -3.7858e-02,\n",
       "          -5.0815e-02, -5.5226e-02, -4.2983e-02, -5.7592e-02, -6.9253e-02,\n",
       "          -6.3775e-02, -6.9458e-02,  7.2511e-02,  4.3604e-02, -2.6877e-02,\n",
       "          -1.1096e-02,  4.3856e-02, -2.0470e-02, -2.6215e-02, -4.0480e-02,\n",
       "          -3.0983e-02, -3.8908e-02, -4.4840e-02, -3.7076e-02, -4.4153e-02,\n",
       "           4.5706e-02,  1.4690e-02, -3.9661e-02, -6.2753e-02, -4.8674e-02,\n",
       "          -5.5976e-02, -2.8874e-02, -5.5633e-02, -2.6556e-02, -3.1353e-02,\n",
       "          -2.9828e-02, -4.0617e-02, -3.8221e-02, -4.7948e-02, -4.2286e-02,\n",
       "          -5.4343e-02, -3.6670e-02,  1.1516e-01,  1.2574e-01, -1.6166e-02,\n",
       "          -5.0977e-02, -4.0474e-02, -4.4173e-02, -3.5620e-02, -3.0940e-02,\n",
       "           6.0096e-02, -3.2650e-02, -2.0136e-02, -3.8184e-02,  7.2825e-03,\n",
       "           9.9650e-02, -4.5380e-02, -2.6495e-02, -6.3710e-02,  1.3577e-01,\n",
       "           5.4456e-02, -7.4386e-02]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
